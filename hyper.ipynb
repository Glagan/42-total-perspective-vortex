{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import mne\n",
    "from mne import Epochs, pick_types, annotations_from_events, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import SPoC\n",
    "from src.CSP import CSP\n",
    "from mne.viz import plot_events, plot_montage\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs, corrmap, Xdawn\n",
    "\n",
    "mne.set_log_level(\"CRITICAL\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the best Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -1.0, 2.0\n",
    "subject = 1\n",
    "subjects = [7, 42]\n",
    "drop_channels = False\n",
    "crop_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "        \"runs\": [3, 7, 11],\n",
    "        \"mapping\": {0: \"rest\", 1: \"left/fist\", 2: \"right/fist\"},\n",
    "        \"event_id\": {\"left/fist\": 1, \"right/fist\": 2},\n",
    "    },\n",
    "    {\n",
    "        \"runs\": [4, 8, 12],\n",
    "        \"mapping\": {0: \"rest\", 1: \"left/imaginefist\", 2: \"right/imaginefist\"},\n",
    "        \"event_id\": {\"left/imaginefist\":1, \"right/imaginefist\":2},\n",
    "    },\n",
    "    {\n",
    "        \"runs\": [5, 9, 13],\n",
    "        \"mapping\": {0: \"rest\", 1: \"top/fists\", 2: \"bottom/feets\"},\n",
    "        \"event_id\": {\"top/fists\": 1, \"bottom/feets\": 2},\n",
    "    },\n",
    "    {\n",
    "        \"runs\": [6, 10, 14],\n",
    "        \"mapping\": {0: \"rest\", 1: \"top/imaginefists\", 2: \"top/imaginefeets\"},\n",
    "        \"event_id\": {\"top/imaginefists\": 1, \"top/imaginefeets\": 2},\n",
    "    },\n",
    "]\n",
    "two_experiments = [\n",
    "    {\n",
    "        \"runs\": [3, 7, 11, 4, 8, 12],\n",
    "        \"mapping\": {0: \"rest\", 1: \"left/fist\", 2: \"right/fist\"},\n",
    "        \"event_id\": {\"left/fist\": 1, \"right/fist\": 2},\n",
    "    },\n",
    "    {\n",
    "        \"runs\": [5, 9, 13, 6, 10, 14],\n",
    "        \"mapping\": {0: \"rest\", 1: \"top/fists\", 2: \"bottom/feets\"},\n",
    "        \"event_id\": {\"top/fists\": 1, \"bottom/feets\": 2},\n",
    "    },\n",
    "]\n",
    "side_experiments = [\n",
    "    {\n",
    "        \"runs\": [3, 7, 11, 4, 8, 12],\n",
    "        \"mapping\": {0: \"rest\", 1: \"left/fist\", 2: \"right/fist\"},\n",
    "        \"event_id\": {\"left/fist\": 1, \"right/fist\": 2},\n",
    "    },\n",
    "    {\n",
    "        \"runs\": [5, 9, 13, 6, 10, 14],\n",
    "        \"mapping\": {0: \"rest\", 1: \"top/fists\", 2: \"bottom/feets\"},\n",
    "        \"event_id\": {\"top/fists\": 1, \"bottom/feets\": 2},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 16 pipelines\n"
     ]
    }
   ],
   "source": [
    "decoders = [\n",
    "    CSP(n_components=2),\n",
    "    CSP(n_components=4),\n",
    "    CSP(n_components=10),\n",
    "    CSP(n_components=15),\n",
    "    # SPoC(n_components=2, log=True, reg='oas', rank='full'),\n",
    "    # SPoC(n_components=4, log=True, reg='oas', rank='full'),\n",
    "    # SPoC(n_components=10, log=True, reg='oas', rank='full'),\n",
    "    # SPoC(n_components=15, log=True, reg='oas', rank='full'),\n",
    "    # PCA(n_components=4),\n",
    "    # PCA(n_components=8),\n",
    "]\n",
    "classifiers = [\n",
    "    SVC(),\n",
    "    LogisticRegression(penalty='l1', solver='liblinear', multi_class='auto'),\n",
    "    RandomForestClassifier(n_estimators=150, random_state=42),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    # Ridge()\n",
    "]\n",
    "\n",
    "pipelines = []\n",
    "for decomposer in decoders:\n",
    "    for classifier in classifiers:\n",
    "        clf = make_pipeline(decomposer, classifier)\n",
    "        pipelines.append(clf)\n",
    "print(f\"Testing {len(pipelines)} pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_raw(raw):\n",
    "    eegbci.standardize(raw)  # set channel names\n",
    "    montage = make_standard_montage(\"biosemi64\")\n",
    "    raw.set_montage(montage, on_missing='ignore')\n",
    "    \n",
    "    # Select channels\n",
    "    if drop_channels:\n",
    "        channels = raw.info[\"ch_names\"] \n",
    "        good_channels = [\n",
    "            \"FC3\",\n",
    "            \"FC1\",\n",
    "            \"FCz\",\n",
    "            \"FC2\",\n",
    "            \"FC4\",\n",
    "            \"C3\",\n",
    "            \"C1\",\n",
    "            \"Cz\",\n",
    "            \"C2\",\n",
    "            \"C4\",\n",
    "            \"CP3\",\n",
    "            \"CP1\",\n",
    "            \"CPz\",\n",
    "            \"CP2\",\n",
    "            \"CP4\",\n",
    "            \"Fpz\",\n",
    "        ]\n",
    "        bad_channels = [x for x in channels if x not in good_channels]\n",
    "        raw.drop_channels(bad_channels)\n",
    "\n",
    "    # Filter\n",
    "    raw.notch_filter(60, method=\"iir\")\n",
    "    raw.filter(7.0, 32.0, fir_design=\"firwin\", skip_by_annotation=\"edge\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_epochs_4_split():\n",
    "    experiments_epochs = []\n",
    "    for experiment_id, experiment in enumerate(experiments):\n",
    "        raw_fnames = [f\"dataset/S{subject:03d}/S{subject:03d}R{run:02d}.edf\" for run in experiment[\"runs\"]]\n",
    "        raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "        events, _ = events_from_annotations(raw, event_id=dict(T0=0, T1=1, T2=2))\n",
    "        annot_from_events = annotations_from_events(\n",
    "            events=events, event_desc=experiment[\"mapping\"], sfreq=raw.info[\"sfreq\"]\n",
    "        )\n",
    "        raw.set_annotations(annot_from_events)\n",
    "        cleanup_raw(raw)\n",
    "\n",
    "        # Read epochs \n",
    "        events, _ = events_from_annotations(raw, event_id=experiment[\"event_id\"])\n",
    "        picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\")\n",
    "        epochs = Epochs(raw, events, experiment[\"event_id\"], tmin, tmax, proj=True, picks=picks, baseline=None, preload=True)\n",
    "        experiments_epochs.append(epochs)\n",
    "    return experiments_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_epochs_2_split():\n",
    "    experiments_epochs = []\n",
    "    for experiment_id, experiment in enumerate(two_experiments):\n",
    "        raw_fnames = [f\"dataset/S{subject:03d}/S{subject:03d}R{run:02d}.edf\" for run in experiment[\"runs\"]]\n",
    "        raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "        events, event_id = events_from_annotations(raw, event_id=dict(T0=0, T1=1, T2=2))\n",
    "        print(event_id)\n",
    "        annot_from_events = annotations_from_events(\n",
    "            events=events, event_desc=experiment[\"mapping\"], sfreq=raw.info[\"sfreq\"]\n",
    "        )\n",
    "        raw.set_annotations(annot_from_events)\n",
    "        cleanup_raw(raw)\n",
    "\n",
    "        # Read epochs \n",
    "        events, _ = events_from_annotations(raw, event_id=experiment[\"event_id\"])\n",
    "        picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\")\n",
    "        epochs = Epochs(raw, events, experiment[\"event_id\"], tmin, tmax, proj=True, picks=picks, baseline=None, preload=True)\n",
    "        experiments_epochs.append(epochs)\n",
    "    return experiments_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_epochs_side_merged():\n",
    "    subject_raws = []\n",
    "    for experiment in side_experiments:\n",
    "        raw_fnames = [f\"dataset/S{subject:03d}/S{subject:03d}R{run:02d}.edf\" for run in experiment[\"runs\"]]\n",
    "        experiment_raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "        events, _ = events_from_annotations(experiment_raw, event_id=dict(T1=1, T2=2))\n",
    "        annot_from_events = annotations_from_events(\n",
    "            events=events, event_desc=experiment[\"mapping\"], sfreq=experiment_raw.info[\"sfreq\"]\n",
    "        )\n",
    "        experiment_raw.set_annotations(annot_from_events)\n",
    "        subject_raws.append(experiment_raw)\n",
    "\n",
    "    raw = concatenate_raws(subject_raws)\n",
    "    cleanup_raw(raw)\n",
    "\n",
    "    # Filter\n",
    "    raw.notch_filter(60, method=\"iir\")\n",
    "    raw.filter(7.0, 32.0, fir_design=\"firwin\", skip_by_annotation=\"edge\") \n",
    "\n",
    "    # Read epochs \n",
    "    events, event_id = events_from_annotations(raw)\n",
    "    picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\")\n",
    "    epochs = Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks, baseline=None, preload=True)\n",
    "    return [epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_epochs_all_merged():\n",
    "    subject_raws = []\n",
    "    for experiment in experiments:\n",
    "        raw_fnames = [f\"dataset/S{subject:03d}/S{subject:03d}R{run:02d}.edf\" for run in experiment[\"runs\"]]\n",
    "        experiment_raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "        events, _ = events_from_annotations(experiment_raw, event_id=dict(T1=1, T2=2))\n",
    "        annot_from_events = annotations_from_events(\n",
    "            events=events, event_desc=experiment[\"mapping\"], sfreq=experiment_raw.info[\"sfreq\"]\n",
    "        )\n",
    "        experiment_raw.set_annotations(annot_from_events)\n",
    "        subject_raws.append(experiment_raw)\n",
    "\n",
    "    raw = concatenate_raws(subject_raws)\n",
    "    cleanup_raw(raw)\n",
    "\n",
    "    # Read epochs \n",
    "    events, event_id = events_from_annotations(raw)\n",
    "    picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\")\n",
    "    epochs = Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks, baseline=None, preload=True)\n",
    "    return [epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_epochs_merged_subjects():\n",
    "    experiments_epochs = []\n",
    "    for experiment_id, experiment in enumerate(experiments):\n",
    "        experiment_raws = []\n",
    "        for subject in subjects:\n",
    "            raw_fnames = [f\"dataset/S{subject:03d}/S{subject:03d}R{run:02d}.edf\" for run in experiment[\"runs\"]]\n",
    "            raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "            events, _ = events_from_annotations(raw, event_id=dict(T0=0, T1=1, T2=2))\n",
    "            annot_from_events = annotations_from_events(\n",
    "                events=events, event_desc=experiment[\"mapping\"], sfreq=raw.info[\"sfreq\"]\n",
    "            )\n",
    "            raw.set_annotations(annot_from_events)\n",
    "            experiment_raws.append(raw)\n",
    "\n",
    "        raw = concatenate_raws(experiment_raws)\n",
    "        cleanup_raw(raw)\n",
    "\n",
    "        # Read epochs \n",
    "        events, _ = events_from_annotations(raw, event_id=experiment[\"event_id\"])\n",
    "        picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\")\n",
    "        epochs = Epochs(raw, events, experiment[\"event_id\"], tmin, tmax, proj=True, picks=picks, baseline=None, preload=True)\n",
    "        experiments_epochs.append(epochs)\n",
    "    return experiments_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Epochs |  45 events (all good), -1 - 2 sec, baseline off, ~10.7 MB, data loaded,\n",
       "  'left/fist': 23\n",
       "  'right/fist': 22>,\n",
       " <Epochs |  45 events (all good), -1 - 2 sec, baseline off, ~10.7 MB, data loaded,\n",
       "  'left/imaginefist': 23\n",
       "  'right/imaginefist': 22>,\n",
       " <Epochs |  45 events (all good), -1 - 2 sec, baseline off, ~10.7 MB, data loaded,\n",
       "  'top/fists': 23\n",
       "  'bottom/feets': 22>,\n",
       " <Epochs |  45 events (all good), -1 - 2 sec, baseline off, ~10.7 MB, data loaded,\n",
       "  'top/imaginefists': 21\n",
       "  'top/imaginefeets': 24>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_epochs = setup_epochs_4_split()\n",
    "# experiments_epochs = setup_epochs_2_split()\n",
    "# experiments_epochs = setup_epochs_side_merged()\n",
    "# experiments_epochs = setup_epochs_all_merged()\n",
    "# experiments_epochs = setup_epochs_merged_subjects()\n",
    "experiments_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pipeline  1/16: Pipeline(steps=[('csp', CSP(n_components=2)), ('svc', SVC())])\n",
      "[Experiment 1] Accuracy: 0.76 (score: 0.56)\n",
      "[Experiment 2] Accuracy: 0.91 (score: 0.63)\n",
      "[Experiment 3] Accuracy: 0.91 (score: 0.92)\n",
      "[Experiment 4] Accuracy: 0.78 (score: 0.89)\n",
      "Accuracy 0.84 ~0.073 | score 0.75 ~0.16\n",
      "\n",
      "# Pipeline  2/16: Pipeline(steps=[('csp', CSP(n_components=2)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(penalty='l1', solver='liblinear'))])\n",
      "[Experiment 1] Accuracy: 0.76 (score: 0.56)\n",
      "[Experiment 2] Accuracy: 0.91 (score: 0.66)\n",
      "[Experiment 3] Accuracy: 0.91 (score: 0.91)\n",
      "[Experiment 4] Accuracy: 0.87 (score: 0.93)\n",
      "Accuracy 0.86 ~0.064 | score 0.76 ~0.16\n",
      "\n",
      "# Pipeline  3/16: Pipeline(steps=[('csp', CSP(n_components=2)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(n_estimators=150, random_state=42))])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.58)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.63)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.88)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.9)\n",
      "Accuracy 1.0 ~0.0 | score 0.75 ~0.14\n",
      "\n",
      "# Pipeline  4/16: Pipeline(steps=[('csp', CSP(n_components=2)),\n",
      "                ('lineardiscriminantanalysis', LinearDiscriminantAnalysis())])\n",
      "[Experiment 1] Accuracy: 0.73 (score: 0.64)\n",
      "[Experiment 2] Accuracy: 0.93 (score: 0.61)\n",
      "[Experiment 3] Accuracy: 0.91 (score: 0.88)\n",
      "[Experiment 4] Accuracy: 0.82 (score: 0.91)\n",
      "Accuracy 0.85 ~0.079 | score 0.76 ~0.13\n",
      "\n",
      "# Pipeline  5/16: Pipeline(steps=[('csp', CSP()), ('svc', SVC())])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.52)\n",
      "[Experiment 2] Accuracy: 0.98 (score: 0.63)\n",
      "[Experiment 3] Accuracy: 0.93 (score: 0.89)\n",
      "[Experiment 4] Accuracy: 0.93 (score: 0.87)\n",
      "Accuracy 0.96 ~0.029 | score 0.73 ~0.16\n",
      "\n",
      "# Pipeline  6/16: Pipeline(steps=[('csp', CSP()),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(penalty='l1', solver='liblinear'))])\n",
      "[Experiment 1] Accuracy: 0.91 (score: 0.58)\n",
      "[Experiment 2] Accuracy: 0.98 (score: 0.57)\n",
      "[Experiment 3] Accuracy: 0.91 (score: 0.91)\n",
      "[Experiment 4] Accuracy: 0.96 (score: 0.93)\n",
      "Accuracy 0.94 ~0.029 | score 0.75 ~0.18\n",
      "\n",
      "# Pipeline  7/16: Pipeline(steps=[('csp', CSP()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(n_estimators=150, random_state=42))])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.51)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.59)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.94)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.91)\n",
      "Accuracy 1.0 ~0.0 | score 0.74 ~0.19\n",
      "\n",
      "# Pipeline  8/16: Pipeline(steps=[('csp', CSP()),\n",
      "                ('lineardiscriminantanalysis', LinearDiscriminantAnalysis())])\n",
      "[Experiment 1] Accuracy: 0.91 (score: 0.63)\n",
      "[Experiment 2] Accuracy: 0.93 (score: 0.48)\n",
      "[Experiment 3] Accuracy: 0.91 (score: 0.89)\n",
      "[Experiment 4] Accuracy: 0.84 (score: 0.92)\n",
      "Accuracy 0.9 ~0.033 | score 0.73 ~0.18\n",
      "\n",
      "# Pipeline  9/16: Pipeline(steps=[('csp', CSP(n_components=10)), ('svc', SVC())])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.6)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.6)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.88)\n",
      "[Experiment 4] Accuracy: 0.96 (score: 0.82)\n",
      "Accuracy 0.99 ~0.019 | score 0.72 ~0.13\n",
      "\n",
      "# Pipeline  10/16: Pipeline(steps=[('csp', CSP(n_components=10)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(penalty='l1', solver='liblinear'))])\n",
      "[Experiment 1] Accuracy: 0.98 (score: 0.51)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.63)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.94)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.91)\n",
      "Accuracy 0.99 ~0.0096 | score 0.75 ~0.18\n",
      "\n",
      "# Pipeline  11/16: Pipeline(steps=[('csp', CSP(n_components=10)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(n_estimators=150, random_state=42))])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.53)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.63)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.97)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.81)\n",
      "Accuracy 1.0 ~0.0 | score 0.74 ~0.17\n",
      "\n",
      "# Pipeline  12/16: Pipeline(steps=[('csp', CSP(n_components=10)),\n",
      "                ('lineardiscriminantanalysis', LinearDiscriminantAnalysis())])\n",
      "[Experiment 1] Accuracy: 0.98 (score: 0.58)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.61)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.89)\n",
      "[Experiment 4] Accuracy: 0.91 (score: 0.78)\n",
      "Accuracy 0.97 ~0.036 | score 0.71 ~0.13\n",
      "\n",
      "# Pipeline  13/16: Pipeline(steps=[('csp', CSP(n_components=15)), ('svc', SVC())])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.59)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.6)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.88)\n",
      "[Experiment 4] Accuracy: 0.96 (score: 0.83)\n",
      "Accuracy 0.99 ~0.019 | score 0.72 ~0.13\n",
      "\n",
      "# Pipeline  14/16: Pipeline(steps=[('csp', CSP(n_components=15)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(penalty='l1', solver='liblinear'))])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.5)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.68)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.94)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.9)\n",
      "Accuracy 1.0 ~0.0 | score 0.76 ~0.18\n",
      "\n",
      "# Pipeline  15/16: Pipeline(steps=[('csp', CSP(n_components=15)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(n_estimators=150, random_state=42))])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.54)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.63)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.79)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.79)\n",
      "Accuracy 1.0 ~0.0 | score 0.69 ~0.1\n",
      "\n",
      "# Pipeline  16/16: Pipeline(steps=[('csp', CSP(n_components=15)),\n",
      "                ('lineardiscriminantanalysis', LinearDiscriminantAnalysis())])\n",
      "[Experiment 1] Accuracy: 1.0 (score: 0.51)\n",
      "[Experiment 2] Accuracy: 1.0 (score: 0.62)\n",
      "[Experiment 3] Accuracy: 1.0 (score: 0.89)\n",
      "[Experiment 4] Accuracy: 1.0 (score: 0.74)\n",
      "Accuracy 1.0 ~0.0 | score 0.69 ~0.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "best_model = None\n",
    "for index, pipeline in enumerate(pipelines):\n",
    "    print(f\"# Pipeline {index + 1: 2}/{len(pipelines)}: {pipeline}\")\n",
    "    pipeline_accuracies = []\n",
    "    pipeline_scores = [] \n",
    "    for exp_index, epochs in enumerate(experiments_epochs): \n",
    "        epochs_data = epochs.get_data()\n",
    "        labels = epochs.events[:, -1]\n",
    "\n",
    "        # monte-carlo cross-validation generator:\n",
    "        cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Display accuracy\n",
    "        epochs_train = epochs.copy()\n",
    "        if crop_train:\n",
    "            epochs_train = epochs_train.crop(1, 2)\n",
    "        score = cross_val_score(pipeline, epochs_train.get_data(), labels, cv=cv, n_jobs=-1, verbose=False).mean()\n",
    "        pipeline.fit(epochs_data, labels)\n",
    "        accuracy = pipeline.score(epochs_data, labels)\n",
    "        print(f\"[Experiment {exp_index + 1}] Accuracy: {accuracy:.2} (score: {score:.2})\")\n",
    "        pipeline_accuracies.append(accuracy) \n",
    "        pipeline_scores.append(score)\n",
    "    accuracy = np.mean(pipeline_accuracies)\n",
    "    score = np.mean(pipeline_scores)\n",
    "    if best_model is None:\n",
    "        best_model = (index, pipeline, accuracy, score)\n",
    "    elif best_model[3] < score:\n",
    "        best_model = (index, pipeline, accuracy, score)\n",
    "    result = f\"Accuracy {accuracy:.2} ~{np.std(pipeline_accuracies):.2} | score {score:.2} ~{np.std(pipeline_scores):.2}\"\n",
    "    all_results.append(result)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.84 ~0.073 | score 0.75 ~0.16\n",
      "Accuracy 0.86 ~0.064 | score 0.76 ~0.16\n",
      "Accuracy 1.0 ~0.0 | score 0.75 ~0.14\n",
      "Accuracy 0.85 ~0.079 | score 0.76 ~0.13\n",
      "Accuracy 0.96 ~0.029 | score 0.73 ~0.16\n",
      "Accuracy 0.94 ~0.029 | score 0.75 ~0.18\n",
      "Accuracy 1.0 ~0.0 | score 0.74 ~0.19\n",
      "Accuracy 0.9 ~0.033 | score 0.73 ~0.18\n",
      "Accuracy 0.99 ~0.019 | score 0.72 ~0.13\n",
      "Accuracy 0.99 ~0.0096 | score 0.75 ~0.18\n",
      "Accuracy 1.0 ~0.0 | score 0.74 ~0.17\n",
      "Accuracy 0.97 ~0.036 | score 0.71 ~0.13\n",
      "Accuracy 0.99 ~0.019 | score 0.72 ~0.13\n",
      "Accuracy 1.0 ~0.0 | score 0.76 ~0.18\n",
      "Accuracy 1.0 ~0.0 | score 0.69 ~0.1\n",
      "Accuracy 1.0 ~0.0 | score 0.69 ~0.14\n"
     ]
    }
   ],
   "source": [
    "for result in all_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, Pipeline(steps=[('csp', CSP(n_components=2)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(penalty='l1', solver='liblinear'))]), 0.861111111111111, 0.7638888888888888)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;csp&#x27;, CSP(n_components=2)),\n",
       "                (&#x27;logisticregression&#x27;,\n",
       "                 LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;csp&#x27;, CSP(n_components=2)),\n",
       "                (&#x27;logisticregression&#x27;,\n",
       "                 LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CSP</label><div class=\"sk-toggleable__content\"><pre>CSP(n_components=2)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('csp', CSP(n_components=2)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(penalty='l1', solver='liblinear'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_model)\n",
    "best_model[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de53c2a5f3662977df581daedca7ffd392916b4fc8ed6ed2b00e9b2d99e942aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
